<!DOCTYPE html>
<html lang="en-us">
  <head>
    <meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1.0">
<meta name="generator" content="Hugo 0.41" />


<title>Spark 快速入门 - Python 语言 - Luoxu34&#39;s Blog</title>
<meta property="og:title" content="Spark 快速入门 - Python 语言 - Luoxu34&#39;s Blog">



  






<link rel="stylesheet" href="https://luoxu34.github.io/css/main.css" media="all">
<link rel="stylesheet" href="//fonts.googleapis.com/css?family=Merriweather:400|Lato:400,400italic,700">

  </head>
  <body>
    <div class="wrapper">
      <header class="header">
        <nav class="nav">
  <a href="https://luoxu34.github.io/" class="nav-logo">
    <img src="https://luoxu34.github.io/images/logo.png" 
         width="50" 
         height="50" 
         alt="Logo">
  </a>

  <ul class="nav-links">
    
    <li><a href="/about/">About</a></li>
    
    <li><a href="https://github.com/luoxu34">GitHub</a></li>
    
    <li><a href="https://luoxu34.github.io/">Home</a></li>
    
  </ul>
</nav>

      </header>


<main class="content" role="main">

  <article class="article">
    
    <span class="article-duration">2 min read</span>
    

    <h1 class="article-title">Spark 快速入门 - Python 语言</h1>

    
    <span class="article-date">June 9, 2018</span>
    

    <div class="article-content">
      

<p><img src="spark-logo.png" alt="spark-logo" /></p>

<p>原文： <a href="http://spark.apache.org/docs/latest/quick-start.html">http://spark.apache.org/docs/latest/quick-start.html</a></p>

<p>译文： <a href="https://luoxu34.github.io/2018/06/09/spark-快速入门---python-语言/">https://luoxu34.github.io/2018/06/09/spark-快速入门---python-语言/</a></p>

<p>译者： <a href="https://github.com/luoxu34">luoxu34</a></p>

<p>时间： 2018/06/08</p>

<p>环境:</p>

<table>
<thead>
<tr>
<th align="left">Name</th>
<th align="left">Version</th>
</tr>
</thead>

<tbody>
<tr>
<td align="left">OS</td>
<td align="left">Ubuntu 18.04 LTS</td>
</tr>

<tr>
<td align="left">JDK</td>
<td align="left">8u172</td>
</tr>

<tr>
<td align="left">Python</td>
<td align="left">3.6.5</td>
</tr>

<tr>
<td align="left">PySpark</td>
<td align="left">2.3.0</td>
</tr>
</tbody>
</table>

<hr />

<blockquote>
<p>需要了解的是，Spark 2.0  之前的版本，主要的编程接口是弹性分布式数据集(Resilient Distributed Dataset, 简称<strong>RDD</strong>)。在 Spark 2.0 之后的版本虽然依旧支持 RDD，但是 RDD 已经被数据集 <strong>Dataset</strong> 所取代，后者不但是强类型的，而且与 RDD 相比具有更多的改进和优化，因此强烈建议使用 Dataset 而不是 RDD。</p>
</blockquote>

<h1 id="1-使用spark-shell进行交互式分析">1. 使用Spark Shell进行交互式分析</h1>

<h2 id="1-1-基础-basics">1.1 基础 Basics</h2>

<p>Spark shell提供了一种学习API的简单方法，它同时也是交互式分析数据的强大工具。
第一步，需要进入 Spark 的交互模式：</p>

<pre><code>$ pyspark
</code></pre>

<p>Spark 的主要抽象是一个称为数据集的分布式项目集合。数据集可以通过 Hadoop 的 InputFormats（例如 HDFS 文件）创建，或通过其他数据集转换而来。由于 Python 的动态特性，我们不要求在 Python 中数据集是强类型。因此，Python 中的所有数据集都是 <code>Dataset[Row]</code>，我们称之为 <code>DataFrame</code>，这与 Pandas 和 R 中的数据框架概念一致。</p>

<p>让我们利用 Spark 源目录中的 README 文件来创建一个新 DataFrame 对象：</p>

<pre><code>&gt;&gt;&gt; textFile = spark.read.text(&quot;README.md&quot;)
</code></pre>

<p>我们可以调用转换函数获得一个新的 DataFrame 对象，或者调用动作函数计算得到返回值。</p>

<p>如果想要了解更多内容，请移步阅读相关的<a href="http://spark.apache.org/docs/latest/api/python/index.html#pyspark.sql.DataFrame">API文档</a>。</p>

<pre><code>&gt;&gt;&gt; textFile.count()  # Number of rows in this DataFrame
126

&gt;&gt;&gt; textFile.first()  # First row in this DataFrame
Row(value=u'# Apache Spark')
</code></pre>

<p>现在让我们将这个 DataFrame 转换成一个新的。通过调用 filter 函数来返回一个包含 Spark 字符串行的新 DataFrame 对象。</p>

<pre><code>&gt;&gt;&gt; linesWithSpark = textFile.filter(textFile.value.contains(&quot;Spark&quot;))
</code></pre>

<p>当然，我们也可以把转换和动作串连起来一并执行：</p>

<pre><code>&gt;&gt;&gt; textFile.filter(textFile.value.contains(&quot;Spark&quot;)).count()
15
</code></pre>

<h2 id="1-2-更多的数据集操作">1.2 更多的数据集操作</h2>

<p>数据集 Dataset 动作(<strong>actions</strong>)和转换(<strong>transformations</strong>)可用于更复杂的计算。例如我们想要获得最多单词的行号，可以这样：</p>

<pre><code>&gt;&gt;&gt; from pyspark.sql.functions import *
&gt;&gt;&gt; textFile.select(size(split(textFile.value, &quot;\s+&quot;)).name(&quot;numWords&quot;)).agg(max(col(&quot;numWords&quot;))).collect()
[Row(max(numWords)=15)]
</code></pre>

<p>首先是map过程：统计每一行的单词数并命名为 &ldquo;numWords&rdquo;，得到一个新的 DataFrame 对象。最后是reduce过程：调用 DataFrame 对象的 agg 方法找到最大的 numWords 值。</p>

<p><code>select</code> 函数和 <code>agg</code> 函数的参数都是列 Column，我们可以通过 <code>df.colName</code> 方式得到 DataFarme 的列。我们也可以导入 pyspark.sql.functions 模块，它提供了很多方便的功能从旧 Column 对象构建一个新的对象。</p>

<p>Spark 可以轻松地实现 Hadoop 的 MapReduce 数据流模式：</p>

<pre><code>&gt;&gt;&gt; wordCounts = textFile.select(explode(split(textFile.value, &quot;\s+&quot;)).alias(&quot;word&quot;)).groupBy(&quot;word&quot;).count()
</code></pre>

<p>为了得到最后的结果，我们需要调用 collect 函数执行计算：</p>

<pre><code>&gt;&gt;&gt; wordCounts.collect()
[Row(word=u'online', count=1), Row(word=u'graphs', count=1), ...]
</code></pre>

<h2 id="1-3-缓存-caching">1.3 缓存 Caching</h2>

<p>Spark 也支持将数据集存入集群范围的内存缓存中。这对于需要进行重复访问的数据非常有用，比如我们需要在一个小的数据集中执行查询操作，或者需要执行一个迭代算法（例如PageRank）。
一个简单的例子，我们可以把 <code>linesWithSpark</code> 数据集进行缓存，就像这样：</p>

<pre><code>&gt;&gt;&gt; linesWithSpark.cache()
&gt;&gt;&gt; linesWithSpark.count()
15
&gt;&gt;&gt; linesWithSpark.count()
15
</code></pre>

<p>使用 Spark 去处理和缓存100行的文本似乎没有什么意义。但实际的情景是，你可以使用这些函数去操作非常大的数据集，即使它们被分布在成百上千的节点上。</p>

<p>你也可以通过 bin/pyspark 连接到集群中以交互方式执行此操作，详情请见<a href="http://spark.apache.org/docs/latest/rdd-programming-guide.html#using-the-shell">RDD编程指南</a>。</p>

<h1 id="2-自包含的应用程序">2. 自包含的应用程序</h1>

<p>现在我们将展示如何使用 Python API（PySpark）编写应用程序。</p>

<p>作为一个例子，我们可以创建一个简单的 Spark 应用程序，<code>SimpleApp.py</code>：</p>

<pre><code>&quot;&quot;&quot;SimpleApp.py&quot;&quot;&quot;
from pyspark.sql import SparkSession

logFile = &quot;YOUR_SPARK_HOME/README.md&quot;  # Should be some file on your system
spark = SparkSession.builder.appName(&quot;SimpleApp&quot;).getOrCreate()
logData = spark.read.text(logFile).cache()

numAs = logData.filter(logData.value.contains('a')).count()
numBs = logData.filter(logData.value.contains('b')).count()

print(&quot;Lines with a: %i, lines with b: %i&quot; % (numAs, numBs))

spark.stop()
</code></pre>

<p>运行程序：</p>

<pre><code># Use the Python interpreter to run your application
$ python SimpleApp.py
...
Lines with a: 46, Lines with b: 23
</code></pre>

<h1 id="3-更多">3. 更多</h1>

<ul>
<li><p>为了深入了解Spark的API，你需要阅读<a href="http://spark.apache.org/docs/latest/rdd-programming-guide.html">RDD编程</a>和<a href="http://spark.apache.org/docs/latest/rdd-programming-guide.html">SQL编程</a>章节。</p></li>

<li><p>如果想要在集群上运行你的程序，需要了解<a href="http://spark.apache.org/docs/latest/rdd-programming-guide.html">部署概览</a>一章。</p></li>

<li><p>最后，Spark自带了几个例子在 <code>examples</code> 目录下，你可以这样去运行它们：</p>

<pre><code># For Scala and Java, use run-example:
./bin/run-example SparkPi

# For Python examples, use spark-submit directly:
./bin/spark-submit examples/src/main/python/pi.py

# For R examples, use spark-submit directly:
./bin/spark-submit examples/src/main/r/dataframe.R
</code></pre></li>
</ul>

    </div>
  </article>

  

</main>

      <footer class="footer">
        <ul class="footer-links">
          <li>
            <a href="https://luoxu34.github.io/index.xml" type="application/rss+xml" target="_blank">RSS feed</a>
          </li>
          <li>
            <a href="https://gohugo.io/" class="footer-links-kudos">Made with <img src="https://luoxu34.github.io/images/hugo-logo.png" width="22" height="22"></a>
          </li>
        </ul>
      </footer>

    </div>

    
  </body>
</html>

